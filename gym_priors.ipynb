{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your main folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/molano/priors_project\n"
     ]
    }
   ],
   "source": [
    "cd '/home/molano/priors_project'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "\n",
    "\n",
    "def update_target_graph(from_scope, to_scope):\n",
    "    \"\"\"\n",
    "    Copies one set of variables to another.\n",
    "    Used to set worker network parameters to those of global network.\n",
    "    \"\"\"\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Discounting function used to calculate discounted returns.\n",
    "    \"\"\"\n",
    "    return lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    \"\"\"\n",
    "    Used to initialize weights for policy and value output layers\n",
    "    \"\"\"\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def look_for_folder(main_folder='priors/', exp=''):\n",
    "    \"\"\"\n",
    "    looks for a given folder and returns it.\n",
    "    If it cannot find it, returns possible candidates\n",
    "    \"\"\"\n",
    "    data_path = ''\n",
    "    possibilities = []\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        ind = root.rfind('/')\n",
    "        possibilities.append(root[ind+1:])\n",
    "        if root[ind+1:] == exp:\n",
    "            data_path = root\n",
    "            break\n",
    "\n",
    "    if data_path == '':\n",
    "        candidates = difflib.get_close_matches(exp, possibilities,\n",
    "                                               n=1, cutoff=0.)\n",
    "        print(exp + ' NOT FOUND IN ' + main_folder)\n",
    "        if len(candidates) > 0:\n",
    "            print('possible candidates:')\n",
    "            print(candidates)\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def list_str(l):\n",
    "    \"\"\"\n",
    "    list to str\n",
    "    \"\"\"\n",
    "    nice_string = str(l[0])\n",
    "    for ind_el in range(1, len(l)):\n",
    "        nice_string += '_'+str(l[ind_el])\n",
    "    return nice_string\n",
    "\n",
    "\n",
    "def num2str(num):\n",
    "    \"\"\"\n",
    "    pass big number to thousands\n",
    "    \"\"\"\n",
    "    return str(int(num/1000))+'K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molano/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def RNN_UGRU(inputs, prev_rewards, a_size, num_units):\n",
    "\n",
    "    # create a UGRNNCell\n",
    "    rnn_cell = tf.contrib.rnn.UGRNNCell(num_units, activation=tf.nn.relu)\n",
    "\n",
    "    # this is the initial state used in the A3C model when training\n",
    "    # or obtaining an action\n",
    "    st_init = np.zeros((1, rnn_cell.state_size), np.float32)\n",
    "\n",
    "    # defining initial state\n",
    "    state_in = tf.placeholder(tf.float32, [1, rnn_cell.state_size])\n",
    "\n",
    "    # reshape inputs size\n",
    "    rnn_in = tf.expand_dims(inputs, [0])\n",
    "\n",
    "    step_size = tf.shape(prev_rewards)[:1]\n",
    "\n",
    "    # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "    outputs, state_out = tf.nn.dynamic_rnn(rnn_cell, rnn_in,\n",
    "                                           initial_state=state_in,\n",
    "                                           sequence_length=step_size,\n",
    "                                           dtype=tf.float32,\n",
    "                                           time_major=False)\n",
    "\n",
    "    rnn_out = tf.reshape(outputs, [-1, num_units])\n",
    "\n",
    "    actions, actions_onehot, policy, value = \\\n",
    "        process_output(rnn_out, outputs, a_size, num_units)\n",
    "\n",
    "    return st_init, state_in, state_out, actions, actions_onehot, policy, value\n",
    "\n",
    "def process_output(rnn_out, outputs, a_size, num_units):\n",
    "    # Actions\n",
    "    actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    actions_onehot = tf.one_hot(actions, a_size, dtype=tf.float32)\n",
    "\n",
    "    # Output layers for policy and value estimations\n",
    "    policy = slim.fully_connected(rnn_out, a_size,\n",
    "                                  activation_fn=tf.nn.softmax,\n",
    "                                  weights_initializer=normalized_columns_initializer(0.01),\n",
    "                                  biases_initializer=None)\n",
    "    value = slim.fully_connected(rnn_out, 1,\n",
    "                                 activation_fn=None,\n",
    "                                 weights_initializer=normalized_columns_initializer(1.0),\n",
    "                                 biases_initializer=None)\n",
    "\n",
    "    return actions, actions_onehot, policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the gym environment (should install and import gym if you have not done so yet).\n",
    "\n",
    "Remove previous gym-priors folder and create new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf gym-priors/\n",
    "! mkdir gym-priors\n",
    "! touch gym-priors/README.md\n",
    "! touch gym-priors/setup.py\n",
    "! mkdir gym-priors/gym_priors/\n",
    "! touch gym-priors/gym_priors/__init__.py\n",
    "! mkdir gym-priors/gym_priors/envs/\n",
    "! touch gym-priors/gym_priors/envs/__init__.py\n",
    "! touch gym-priors/gym_priors/envs/priors_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gym-priors/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gym-priors/setup.py\n",
    "from setuptools import setup\n",
    "print('setup!')\n",
    "setup(name='gym_priors',\n",
    "      version='0.0.1',\n",
    "      install_requires=['gym'])  # And any other dependencies required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gym-priors/gym_priors/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gym-priors/gym_priors/__init__.py\n",
    "from gym.envs.registration import register\n",
    "register(\n",
    "    id='priors-v0',\n",
    "    entry_point='gym_priors.envs:PriorsEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gym-priors/gym_priors/envs/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gym-priors/gym_priors/envs/__init__.py\n",
    "from gym_priors.envs.priors_env import PriorsEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting gym-priors/gym_priors/envs/priors_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile gym-priors/gym_priors/envs/priors_env.py\n",
    "import sys\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "\n",
    "# from gym.utils import seeding\n",
    "\n",
    "\n",
    "class PriorsEnv(gym.Env):\n",
    "    metadata = {}\n",
    "\n",
    "    def __init__(self, exp_dur=100, trial_dur=10, upd_net=5,\n",
    "                 rep_prob=(.2, .8), rewards=(0.1, -0.1, 1.0, -1.0),\n",
    "                 env_seed='0', block_dur=200, stim_ev=0.5, folder=None):\n",
    "        print('init environment!')\n",
    "        # exp. duration (training will consist in several experiments)\n",
    "        self.exp_dur = exp_dur\n",
    "        # num steps per trial\n",
    "        self.trial_dur = trial_dur\n",
    "        # rewards given for: stop fixating, keep fixating, correct, wrong\n",
    "        self.rewards = rewards\n",
    "        # number of trials per blocks\n",
    "        self.block_dur = block_dur\n",
    "        # stimulus evidence: one stimulus is always N(1,1), the mean of\n",
    "        # the other is drawn from a uniform distrib.=U(stim_ev,1).\n",
    "        # stim_ev must then be between 0 and 1 and the higher it is\n",
    "        # the more difficult will be the task\n",
    "        self.stim_ev = stim_ev\n",
    "        # prob. of repeating the stimuli in the positions of previous trial\n",
    "        self.rep_prob = rep_prob\n",
    "        # model instance\n",
    "        self.env_seed = env_seed\n",
    "        # folder to save data\n",
    "        self.folder = folder\n",
    "        # update parameters\n",
    "        self.upd_net = upd_net\n",
    "\n",
    "        # num actions\n",
    "        self.num_actions = 3\n",
    "        self.action_space = spaces.Discrete(self.num_actions)\n",
    "        # position of the first stimulus\n",
    "        self.stms_pos_new_trial = np.random.choice([0, 1])\n",
    "        # keeps track of the repeating prob of the current block\n",
    "        self.curr_rep_prob = np.random.choice([0, 1])\n",
    "        # position of the stimuli\n",
    "        self.stm_pos_new_trial = 0\n",
    "        # steps counter\n",
    "        self.timestep = 0\n",
    "        # initialize ground truth state [stim1 mean, stim2 mean, fixation])\n",
    "        # the network has to output the action corresponding to the stim1 mean\n",
    "        # that will be always 1.0 (I just initialize here at 0 for convinience)\n",
    "        self.int_st = np.array([0, 0, -1])\n",
    "        # accumulated evidence\n",
    "        self.evidence = 0\n",
    "        # observation space\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n",
    "                                            shape=(3, ), dtype=np.float32)\n",
    "        # number of trials\n",
    "        self.num_tr = 0\n",
    "\n",
    "        # trial data to save\n",
    "        # stimulus evidence\n",
    "        self.ev_mat = []\n",
    "        # position of stimulus 1\n",
    "        self.stm_pos = []\n",
    "        # performance\n",
    "        self.perf_mat = []\n",
    "        # summed activity across the trial\n",
    "        self.action = []\n",
    "\n",
    "    def update_params(self, exp_dur=10**4, trial_dur=10, upd_net=5,\n",
    "                 rep_prob=(.2, .8), rewards=(0.1, -0.1, 1.0, -1.0),\n",
    "                 env_seed='0', block_dur=200, stim_ev=0.5, folder='', seed=0):\n",
    "        \"\"\"\n",
    "        this function should be run after creating an environment to set the\n",
    "        main parameters.\n",
    "        There does not seem to be easy way of passing those parameters\n",
    "        when using the make function of the gym toolbox, so this is a way\n",
    "        around to set the parameters.\n",
    "        \"\"\"\n",
    "        # we update the network every x trials\n",
    "        self.upd_net = upd_net or self.upd_net\n",
    "        # exp. duration (num. trials; training consists in several exps)\n",
    "        self.exp_dur = exp_dur or self.exp_dur\n",
    "        # num steps per trial\n",
    "        self.trial_dur = trial_dur or self.trial_dur\n",
    "        # rewards given for: stop fixating, keep fixating, correct, wrong\n",
    "        self.rewards = rewards or self.rewards\n",
    "        # number of trials per blocks\n",
    "        self.block_dur = block_dur or self.block_dur\n",
    "        # stimulus evidence\n",
    "        stim_ev = stim_ev or self.stim_ev\n",
    "        self.stim_ev = np.max([stim_ev, 10e-5])\n",
    "        # prob. of repeating the stimuli in the positions of previous trial\n",
    "        self.rep_prob = rep_prob or self.rep_prob\n",
    "        # model seed\n",
    "        self.env_seed = seed or self.env_seed\n",
    "        # folder where data will be saved\n",
    "        aux_folder = folder or self.folder\n",
    "        if aux_folder is not None:\n",
    "            self.sv_data = True\n",
    "        else:\n",
    "            self.sv_data = False\n",
    "        print('--------------- Priors experiment ---------------')\n",
    "        print('Duration of each experiment (in trials): ' +\n",
    "              str(self.exp_dur))\n",
    "        print('Duration of each trial (in steps): ' + str(self.trial_dur))\n",
    "        print('Rewards: ' + str(self.rewards))\n",
    "        print('Duration of each block (in trials): ' + str(self.block_dur))\n",
    "        print('Repeating probabilities of each block: ' + str(self.rep_prob))\n",
    "        print('Stim evidence: ' + str(self.stim_ev))\n",
    "        print('Saving folder: ' + str(self.folder))\n",
    "        print('--------------- ----------------- ---------------')\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        receives an action and returns a reward, a state and flag variables\n",
    "        that indicate whether to start a new trial and whether to update\n",
    "        the network\n",
    "        \"\"\"\n",
    "        new_trial = True\n",
    "        correct = False\n",
    "        done = False\n",
    "        # decide which reward and state (new_trial, correct) we are in\n",
    "        if self.timestep < self.trial_dur:\n",
    "            if (self.int_st[action] != -1).all():\n",
    "                reward = self.rewards[0]\n",
    "            else:\n",
    "                # don't abort the trial even if the network stops fixating\n",
    "                reward = self.rewards[1]\n",
    "\n",
    "            new_trial = False\n",
    "\n",
    "        else:\n",
    "            if (self.int_st[action] == 1.0).all():\n",
    "                reward = self.rewards[2]\n",
    "                correct = True\n",
    "            else:\n",
    "                reward = self.rewards[3]\n",
    "\n",
    "        info = {'perf': correct, 'ev': self.evidence}\n",
    "\n",
    "        if new_trial:\n",
    "            if self.sv_data:\n",
    "                # keep main variables of the trial\n",
    "                self.stm_pos.append(self.stms_pos_new_trial)\n",
    "                self.perf_mat.append(correct)\n",
    "                self.action.append(action)\n",
    "                self.ev_mat.append(self.evidence)\n",
    "            new_st = self.new_trial()\n",
    "            # check if it is time to update the network\n",
    "            done = ((self.num_tr-1) % self.exp_dur == 0) and (self.num_tr != 1)\n",
    "            # check if it is time to save the trial-to-trial data\n",
    "            if self.sv_data and self.num_tr % 10000 == 0:\n",
    "                self.save_trials_data()\n",
    "                self.output_stats()\n",
    "        else:\n",
    "            new_st = self.get_state()\n",
    "\n",
    "        return new_st, reward, done, info\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Outputs a new observation using stim 1 and 2 means.\n",
    "        It also outputs a fixation signal that is always -1 except at the\n",
    "        end of the trial that is 0\n",
    "        \"\"\"\n",
    "        self.timestep += 1\n",
    "        # if still in the integration period present a new observation\n",
    "        if self.timestep < self.trial_dur:\n",
    "            self.state = [np.random.normal(self.int_st[0]),\n",
    "                          np.random.normal(self.int_st[1]), -1]\n",
    "        else:\n",
    "            self.state = [0, 0, 0]\n",
    "\n",
    "        # update evidence\n",
    "        self.evidence += self.state[0]-self.state[1]\n",
    "\n",
    "        return np.reshape(self.state, (3, ))\n",
    "\n",
    "    def new_trial(self):\n",
    "        \"\"\"\n",
    "        this function creates a new trial, deciding the amount of coherence\n",
    "        (through the mean of stim 2) and the position of stim 1. Once it has\n",
    "        done this it calls get_state to get the first observation of the trial\n",
    "        \"\"\"\n",
    "        self.num_tr += 1\n",
    "        self.timestep = 0\n",
    "        self.evidence = 0\n",
    "        # this are the means of the two stimuli\n",
    "        stim1 = 1.0\n",
    "        stim2 = np.random.uniform(1-self.stim_ev, 1)\n",
    "        assert stim2 != 1.0\n",
    "        self.choices = [stim1, stim2]\n",
    "\n",
    "        # decide the position of the stims\n",
    "        # if the block is finished update the prob of repeating\n",
    "        if self.num_tr % self.block_dur == 0:\n",
    "            self.curr_rep_prob = int(not self.curr_rep_prob)\n",
    "\n",
    "        # flip a coin\n",
    "        repeat = np.random.uniform() < self.rep_prob[self.curr_rep_prob]\n",
    "        if not repeat:\n",
    "            self.stms_pos_new_trial = not(self.stms_pos_new_trial)\n",
    "\n",
    "        aux = [self.choices[x] for x in [int(self.stms_pos_new_trial),\n",
    "                                         int(not self.stms_pos_new_trial)]]\n",
    "\n",
    "        self.int_st = np.concatenate((aux, np.array([-1])))\n",
    "\n",
    "        # get state\n",
    "        s = self.get_state()\n",
    "\n",
    "        return s\n",
    "\n",
    "    def save_trials_data(self):\n",
    "        \"\"\"\n",
    "        save trial-to-trial data for:\n",
    "        evidence, stim postion, action taken and outcome\n",
    "        \"\"\"\n",
    "        # Periodically save model trials statistics.\n",
    "        data = {'stims_position': self.stm_pos,\n",
    "                'action': self.action,\n",
    "                'performance': self.perf_mat,\n",
    "                'evidence': self.ev_mat}\n",
    "        np.savez(self.folder + '/trials_stats_' +\n",
    "                 str(self.env_seed) + '_' + str(self.num_tr) + '.npz', **data)\n",
    "\n",
    "    def reset(self):\n",
    "        return self.new_trial()\n",
    "\n",
    "    def output_stats(self):\n",
    "        \"\"\"\n",
    "        plot temporary learning and bias curves\n",
    "        \"\"\"\n",
    "        # add current path to sys.path so as to import analyses_priors\n",
    "        sys.path.append(os.path.dirname(os.path.realpath(__file__)))\n",
    "        import analyses_priors as ap\n",
    "        aux_shape = (1, len(self.ev_mat))\n",
    "        # plot psycho. curves\n",
    "        per = 20000\n",
    "        ev = np.reshape(self.ev_mat, aux_shape).copy()\n",
    "        ev = ev[np.max([0, len(ev)-per]):]\n",
    "        perf = np.reshape(self.perf_mat,\n",
    "                          aux_shape).copy()\n",
    "        perf = perf[np.max([0, len(perf)-per]):]\n",
    "        action = np.reshape(self.action, aux_shape).copy()\n",
    "        action = action[np.max([0, len(action)-per]):]\n",
    "        stim_pos = np.reshape(self.stm_pos,\n",
    "                              aux_shape).copy()\n",
    "        stim_pos = stim_pos[np.max([0, len(stim_pos)-per]):]\n",
    "        ap.plot_psychometric_curves(ev, perf, action, blk_dur=self.block_dur,\n",
    "                                    figs=True, folder=self.folder,\n",
    "                                    name='psycho_'+str(self.num_tr))\n",
    "        # plot learning\n",
    "        ev = np.reshape(self.ev_mat, aux_shape).copy()\n",
    "        perf = np.reshape(self.perf_mat,\n",
    "                          aux_shape).copy()\n",
    "        action = np.reshape(self.action, aux_shape).copy()\n",
    "        stim_pos = np.reshape(self.stm_pos,\n",
    "                              aux_shape).copy()\n",
    "        ap.plot_learning(perf, ev, stim_pos, action, folder=self.folder,\n",
    "                         name='', save_fig=True, view_fig=True)\n",
    "\n",
    "    def render():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/molano/priors_project/gym-priors\n"
     ]
    }
   ],
   "source": [
    "cd gym-priors/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/molano/priors_project/gym-priors\n",
      "Requirement already satisfied: gym in /home/molano/anaconda3/lib/python3.6/site-packages (from gym-priors==0.0.1) (0.10.9)\n",
      "Requirement already satisfied: six in /home/molano/anaconda3/lib/python3.6/site-packages (from gym->gym-priors==0.0.1) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /home/molano/anaconda3/lib/python3.6/site-packages (from gym->gym-priors==0.0.1) (1.14.3)\n",
      "Requirement already satisfied: scipy in /home/molano/anaconda3/lib/python3.6/site-packages (from gym->gym-priors==0.0.1) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.0 in /home/molano/anaconda3/lib/python3.6/site-packages (from gym->gym-priors==0.0.1) (2.18.4)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /home/molano/anaconda3/lib/python3.6/site-packages (from gym->gym-priors==0.0.1) (1.3.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/molano/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym->gym-priors==0.0.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/molano/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym->gym-priors==0.0.1) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/molano/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym->gym-priors==0.0.1) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/molano/anaconda3/lib/python3.6/site-packages (from requests>=2.0->gym->gym-priors==0.0.1) (2018.4.16)\n",
      "Requirement already satisfied: future in /home/molano/anaconda3/lib/python3.6/site-packages (from pyglet>=1.2.0->gym->gym-priors==0.0.1) (0.17.1)\n",
      "\u001b[31mtensorflow-gpu 1.11.0 has requirement tensorboard<1.12.0,>=1.11.0, but you'll have tensorboard 1.12.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: gym-priors\n",
      "  Found existing installation: gym-priors 0.0.1\n",
      "    Uninstalling gym-priors-0.0.1:\n",
      "      Successfully uninstalled gym-priors-0.0.1\n",
      "  Running setup.py develop for gym-priors\n",
      "Successfully installed gym-priors\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mgym_priors\u001b[0m/  \u001b[01;34mgym_priors.egg-info\u001b[0m/  README.md  setup.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/molano/priors_project\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosyne_priors.ipynb  \u001b[0m\u001b[01;34mgym-priors\u001b[0m/  gym_priors.ipynb  \u001b[01;34mpriors\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, a_size, state_size, scope, trainer, num_units, network):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Input and visual encoding layers\n",
    "            self.st = tf.placeholder(shape=[None, 1, state_size, 1],\n",
    "                                     dtype=tf.float32)\n",
    "            self.prev_rewards = tf.placeholder(shape=[None, 1],\n",
    "                                               dtype=tf.float32)\n",
    "            self.prev_actions = tf.placeholder(shape=[None],\n",
    "                                               dtype=tf.int32)\n",
    "\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions, a_size,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "            hidden = tf.concat([slim.flatten(self.st), self.prev_rewards,\n",
    "                                self.prev_actions_onehot], 1)\n",
    "\n",
    "            # call RNN network\n",
    "            if network == 'relu':\n",
    "                net = RNN_ReLU\n",
    "            elif network == 'lstm':\n",
    "                net = RNN\n",
    "            elif network == 'gru':\n",
    "                net = RNN_GRU\n",
    "            elif network == 'ugru':\n",
    "                net = RNN_UGRU\n",
    "            else:\n",
    "                raise ValueError('Unknown network')\n",
    "\n",
    "            self.st_init, self.st_in, self.st_out, self.actions,\\\n",
    "                self.actions_onehot, self.policy, self.value =\\\n",
    "                net(hidden, self.prev_rewards, a_size, num_units)\n",
    "\n",
    "            # Only the worker network needs ops for loss functions\n",
    "            # and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "                self.resp_outputs = \\\n",
    "                    tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                # Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(\n",
    "                        tf.square(self.target_v -\n",
    "                                  tf.reshape(self.value, [-1])))\n",
    "                self.entropy = - tf.reduce_sum(\n",
    "                        self.policy * tf.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(\n",
    "                        tf.log(self.resp_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss +\\\n",
    "                    self.policy_loss -\\\n",
    "                    self.entropy * 0.05\n",
    "\n",
    "                # Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(\n",
    "                        tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss, local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads, self.grad_norms =\\\n",
    "                    tf.clip_by_global_norm(self.gradients, 999.0)\n",
    "\n",
    "                # Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(\n",
    "                        tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(\n",
    "                        zip(grads, global_vars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Worker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, game, name, a_size, state_size, trainer,\n",
    "                 model_path, global_epss, data_path, num_units, network):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name\n",
    "        self.folder = './' + data_path + '/trains/train_' + str(self.number)\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_epss = global_epss\n",
    "        self.increment = self.global_epss.assign_add(1)\n",
    "        self.network = network\n",
    "        self.eps_rewards = []\n",
    "        self.eps_mean_values = []\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(self.folder)\n",
    "\n",
    "        # Create the local copy of the network and the tensorflow op\n",
    "        # to copy global parameters to local network\n",
    "        self.local_AC = AC_Network(a_size, state_size, self.name, trainer,\n",
    "                                   num_units, network)\n",
    "        self.update_local_ops = update_target_graph('global', self.name)\n",
    "        self.env = game\n",
    "\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        states = rollout[:, 0]\n",
    "        actions = rollout[:, 1]\n",
    "        rewards = rollout[:, 2]\n",
    "\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        values = rollout[:, 3]\n",
    "\n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        # Here we take the rewards and values from the rollout, and use them to\n",
    "        # generate the advantage and discounted returns.\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards +\\\n",
    "            gamma * self.value_plus[1:] -\\\n",
    "            self.value_plus[:-1]\n",
    "        advantages = discount(advantages, gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.st_init\n",
    "        if self.network == 'lstm':\n",
    "            feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                         self.local_AC.state: np.stack(states, axis=0),\n",
    "                         self.local_AC.prev_rewards: np.vstack(prev_rewards),\n",
    "                         self.local_AC.prev_actions: prev_actions,\n",
    "                         self.local_AC.actions: actions,\n",
    "                         self.local_AC.advantages: advantages,\n",
    "                         self.local_AC.state_in[0]: rnn_state[0],\n",
    "                         self.local_AC.state_in[1]: rnn_state[1]}\n",
    "        elif (self.network == 'relu') or\\\n",
    "             (self.network == 'gru') or\\\n",
    "             (self.network == 'ugru'):\n",
    "            feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                         self.local_AC.st: np.stack(states, axis=0),\n",
    "                         self.local_AC.prev_rewards: np.vstack(prev_rewards),\n",
    "                         self.local_AC.prev_actions: prev_actions,\n",
    "                         self.local_AC.actions: actions,\n",
    "                         self.local_AC.advantages: advantages,\n",
    "                         self.local_AC.st_in: rnn_state}\n",
    "\n",
    "        v_l, p_l, e_l, g_n, v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "                                               self.local_AC.policy_loss,\n",
    "                                               self.local_AC.entropy,\n",
    "                                               self.local_AC.grad_norms,\n",
    "                                               self.local_AC.var_norms,\n",
    "                                               self.local_AC.apply_grads],\n",
    "                                              feed_dict=feed_dict)\n",
    "        aux = len(rollout)\n",
    "        return v_l / aux, p_l / aux, e_l / aux, g_n, v_n\n",
    "\n",
    "    def work(self, gamma, sess, coord, saver, train, exp_dur):\n",
    "        eps_count = sess.run(self.global_epss)\n",
    "        num_eps_tr_stats = int(1000/self.env.upd_net)\n",
    "        num_epss_end = int(exp_dur/self.env.upd_net)\n",
    "        num_epss_save_model = int(5000/self.env.upd_net)\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                eps_buffer = []\n",
    "                eps_values = []\n",
    "                eps_reward = 0\n",
    "                eps_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "\n",
    "                # get first state\n",
    "                s = self.env.new_trial()\n",
    "\n",
    "                rnn_state = self.local_AC.st_init\n",
    "                net_smmd_act = np.zeros_like(rnn_state)\n",
    "                while not d:\n",
    "                    if self.network == 'lstm':\n",
    "                        feed_dict = {\n",
    "                                    self.local_AC.state: [s],\n",
    "                                    self.local_AC.prev_rewards: [[r]],\n",
    "                                    self.local_AC.prev_actions: [a],\n",
    "                                    self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                    self.local_AC.state_in[1]: rnn_state[1]}\n",
    "                    elif (self.network == 'relu') or\\\n",
    "                         (self.network == 'gru') or\\\n",
    "                         (self.network == 'ugru'):\n",
    "                        feed_dict = {\n",
    "                                    self.local_AC.st: [s],\n",
    "                                    self.local_AC.prev_rewards: [[r]],\n",
    "                                    self.local_AC.prev_actions: [a],\n",
    "                                    self.local_AC.st_in: rnn_state}\n",
    "\n",
    "                    # Take an action using probs from policy network output\n",
    "                    a_dist, v, rnn_state_new = sess.run(\n",
    "                                                        [self.local_AC.policy,\n",
    "                                                         self.local_AC.value,\n",
    "                                                         self.local_AC.st_out],\n",
    "                                                        feed_dict=feed_dict)\n",
    "\n",
    "                    a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    rnn_state = rnn_state_new\n",
    "                    net_smmd_act += rnn_state_new\n",
    "                    aux = np.floor(self.env.num_tr/self.env.num_tr_svd)\n",
    "                    if aux % self.env.sv_pts_stp == 0:\n",
    "                        network_activity = rnn_state_new\n",
    "                    else:\n",
    "                        network_activity = []\n",
    "                    # new_state, reward, update_net, new_trial\n",
    "                    s1, r, d, nt = self.env.step(a)\n",
    "                    # save samples for training the network later\n",
    "                    eps_buffer.append([s, a, r, v[0, 0]])\n",
    "                    eps_values.append(v[0, 0])\n",
    "                    eps_reward += r\n",
    "                    total_steps += 1\n",
    "                    eps_step_count += 1\n",
    "                    # store the summed activity at the end of the trial\n",
    "                    if nt:\n",
    "                        self.env.net_smmd_act.append(net_smmd_act)\n",
    "                        net_smmd_act = np.zeros_like(rnn_state)\n",
    "                        self.env.save_trials_data()\n",
    "                    if not d:\n",
    "                        if nt:\n",
    "                            s = self.env.new_trial()\n",
    "                        else:\n",
    "                            s = s1\n",
    "\n",
    "                self.eps_rewards.append(eps_reward)\n",
    "                self.eps_mean_values.append(np.mean(eps_values))\n",
    "\n",
    "                # Update the network using the experience buffer\n",
    "                # at the end of the episode\n",
    "                if len(eps_buffer) != 0 and train:\n",
    "                    v_l, p_l, e_l, g_n, v_n = \\\n",
    "                        self.train(eps_buffer, sess, gamma, 0.0)\n",
    "\n",
    "                # Periodically save model parameters and summary statistics.\n",
    "                if eps_count % num_eps_tr_stats == 0 and eps_count != 0:\n",
    "                    if eps_count % num_epss_save_model == 0 and\\\n",
    "                       self.name == 'worker_0' and\\\n",
    "                       train and\\\n",
    "                       len(self.eps_rewards) != 0:\n",
    "                        saver.save(sess, self.model_path +\n",
    "                                   '/model-' + str(eps_count) + '.cptk')\n",
    "\n",
    "                    mean_tr_dur = np.mean(self.env.dur_tr[-10:])\n",
    "                    mean_reward = np.mean(self.eps_rewards[-10:])\n",
    "                    mean_value = np.mean(self.eps_mean_values[-10:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/trial_duration',\n",
    "                                      simple_value=float(mean_tr_dur))\n",
    "                    summary.value.add(tag='Perf/Reward',\n",
    "                                      simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Value',\n",
    "                                      simple_value=float(mean_value))\n",
    "\n",
    "                    performance_aux = np.vstack(np.array(self.env.perf_mat))\n",
    "\n",
    "                    for ind_crr in range(performance_aux.shape[1]):\n",
    "                        mean_performance = np.mean(performance_aux[:, ind_crr])\n",
    "                        summary.value.add(tag='Perf/Perf_' + str(ind_crr),\n",
    "                                          simple_value=float(mean_performance))\n",
    "\n",
    "                    if train:\n",
    "                        summary.value.add(tag='Losses/Value Loss',\n",
    "                                          simple_value=float(v_l))\n",
    "                        summary.value.add(tag='Losses/Policy Loss',\n",
    "                                          simple_value=float(p_l))\n",
    "                        summary.value.add(tag='Losses/Entropy',\n",
    "                                          simple_value=float(e_l))\n",
    "                        summary.value.add(tag='Losses/Grad Norm',\n",
    "                                          simple_value=float(g_n))\n",
    "                        summary.value.add(tag='Losses/Var Norm',\n",
    "                                          simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, eps_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "\n",
    "                eps_count += 1\n",
    "                if eps_count > num_epss_end:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import os\n",
    "import gym\n",
    "import gym_priors\n",
    "\n",
    "def main_priors(load_model=False, train=True, gamma=.8, up_net=5,\n",
    "                trial_dur=10, rep_prob=(0.2, 0.8), exp_dur=1000,\n",
    "                rewards=(-0.1, 0.0, 1.0, -1.0), block_dur=200,\n",
    "                num_units=32, stim_ev=.3, network='ugru',\n",
    "                learning_rate=1e-3, instance=0):\n",
    "    a_size = 3  # number of actions\n",
    "    state_size = a_size  # number of inputs\n",
    "    if train:\n",
    "        test_flag = ''\n",
    "    else:\n",
    "        test_flag = '_test'\n",
    "    data_path = 'priors/' + 'trial_dur_' + str(trial_dur) +\\\n",
    "        '_rep_prob_' + str(list_str(rep_prob)) +\\\n",
    "        '_rewards_' + str(list_str(rewards)) +\\\n",
    "        '_block_dur_' + str(block_dur) + '_stimEv_' + str(stim_ev) +\\\n",
    "        '_gamma_' + str(gamma) + '_num_units_' + str(num_units) +\\\n",
    "        '_up_net_' + str(up_net) + '_network_' \\\n",
    "        + str(network) + '_' + str(instance) + test_flag + '/'\n",
    "\n",
    "    data = {'trial_dur': trial_dur, 'rep_prob': rep_prob,\n",
    "            'rewards': rewards, 'stim_ev': stim_ev,\n",
    "            'block_dur': block_dur, 'gamma': gamma, 'num_units': num_units,\n",
    "            'up_net': up_net, 'network': network}\n",
    "\n",
    "    model_path = './' + data_path + '/model_meta_context'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    np.savez(data_path + '/experiment_setup.npz', **data)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        global_episodes = tf.Variable(0, dtype=tf.int32,\n",
    "                                      name='global_episodes',\n",
    "                                      trainable=False)\n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        AC_Network(a_size, state_size, 'global',\n",
    "                             None, num_units, network)  # Generate global net\n",
    "        # Set workers to number of available CPU threads\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "        workers = []\n",
    "        # Create worker classes\n",
    "        for i in range(num_workers):\n",
    "            env = gym.make('priors-v0')\n",
    "            saving_path = './' + data_path + '/trains/train_' + str(i)\n",
    "            env.update_params(upd_net=up_net, trial_dur=trial_dur,\n",
    "                              rep_prob=rep_prob, rewards=rewards,\n",
    "                              block_dur=block_dur, stim_ev=stim_ev,\n",
    "                              folder=saving_path)\n",
    "\n",
    "            workers.append(Worker(env, i, a_size, state_size,\n",
    "                            trainer, model_path, global_episodes,\n",
    "                            data_path, num_units, network))\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        if load_model:\n",
    "            print('Loading Model...')\n",
    "            print(model_path)\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        worker_threads = []\n",
    "        for worker in workers:\n",
    "            worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
    "            thread = threading.Thread(target=(worker_work))\n",
    "            thread.start()\n",
    "            worker_threads.append(thread)\n",
    "        coord.join(worker_threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init environment!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'list_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-eaf2e82ef0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_dur\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstim_ev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ugru'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 learning_rate=1e-3, instance=123)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-543d62e8df97>\u001b[0m in \u001b[0;36mmain_priors\u001b[0;34m(load_model, train, gamma, up_net, trial_dur, rep_prob, exp_dur, rewards, block_dur, num_units, stim_ev, network, learning_rate, instance)\u001b[0m\n\u001b[1;32m     48\u001b[0m                               \u001b[0mrep_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrep_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                               \u001b[0mblock_dur\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblock_dur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstim_ev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstim_ev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                               folder=saving_path)\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             workers.append(Worker(env, i, a_size, state_size,\n",
      "\u001b[0;32m~/priors_project/gym-priors/gym_priors/envs/priors_env.py\u001b[0m in \u001b[0;36mupdate_params\u001b[0;34m(self, exp_dur, trial_dur, upd_net, rep_prob, rewards, env_seed, block_dur, stim_ev, folder, seed)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;34m'_r_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'_bd_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_dur\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ev_'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstim_ev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_str' is not defined"
     ]
    }
   ],
   "source": [
    "main_priors(load_model=False, train=True, gamma=.8, up_net=5,\n",
    "                trial_dur=10, rep_prob=(0.2, 0.8),\n",
    "                rewards=(-0.1, 0.0, 1.0, -1.0), block_dur=200,\n",
    "                num_units=32, stim_ev=.3, network='ugru',\n",
    "                learning_rate=1e-3, instance=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('priors-v0')\n",
    "env.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12.2333px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "262.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "325.85px",
    "left": "1499.42px",
    "right": "20px",
    "top": "120px",
    "width": "281.767px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
