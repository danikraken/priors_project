{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import lfilter\n",
    "\n",
    "\n",
    "def update_target_graph(from_scope,to_scope):\n",
    "    \"\"\"\n",
    "    Copies one set of variables to another.\n",
    "    Used to set worker network parameters to those of global network.\n",
    "    \"\"\"\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "    op_holder = []\n",
    "    for from_var,to_var in zip(from_vars,to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def discount(x, gamma):\n",
    "    \"\"\"\n",
    "    Discounting function used to calculate discounted returns.\n",
    "    \"\"\"\n",
    "    return lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "def normalized_columns_initializer(std=1.0):\n",
    "    \"\"\"\n",
    "    Used to initialize weights for policy and value output layers\n",
    "    \"\"\"\n",
    "    def _initializer(shape, dtype=None, partition_info=None):\n",
    "        out = np.random.randn(*shape).astype(np.float32)\n",
    "        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n",
    "        return tf.constant(out)\n",
    "    return _initializer\n",
    "\n",
    "def look_for_folder(main_folder='priors/', exp=''):\n",
    "    \"\"\"\n",
    "    looks for a given folder and returns it.\n",
    "    If it cannot find it, returns possible candidates\n",
    "    \"\"\"\n",
    "    data_path = ''\n",
    "    possibilities = []\n",
    "    for root, dirs, files in os.walk(main_folder):\n",
    "        ind = root.rfind('/')\n",
    "        possibilities.append(root[ind+1:])\n",
    "        if root[ind+1:] == exp:\n",
    "            data_path = root\n",
    "            break\n",
    "\n",
    "    if data_path == '':\n",
    "        candidates = difflib.get_close_matches(exp, possibilities,\n",
    "                                               n=1, cutoff=0.)\n",
    "        print(exp + ' NOT FOUND IN ' + main_folder)\n",
    "        if len(candidates) > 0:\n",
    "            print('possible candidates:')\n",
    "            print(candidates)\n",
    "\n",
    "    return data_path\n",
    "\n",
    "\n",
    "def list_str(l):\n",
    "    \"\"\"\n",
    "    list to str\n",
    "    \"\"\"\n",
    "    nice_string = str(l[0])\n",
    "    for ind_el in range(1, len(l)):\n",
    "        nice_string += '_'+str(l[ind_el])\n",
    "    return nice_string\n",
    "\n",
    "\n",
    "def num2str(num):\n",
    "    \"\"\"\n",
    "    pass big number to thousands\n",
    "    \"\"\"\n",
    "    return str(int(num/1000))+'K'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molano/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "def RNN_UGRU(inputs, prev_rewards, a_size, num_units):\n",
    "\n",
    "    # create a UGRNNCell\n",
    "    rnn_cell = tf.contrib.rnn.UGRNNCell(num_units, activation=tf.nn.relu)\n",
    "\n",
    "    # this is the initial state used in the A3C model when training\n",
    "    # or obtaining an action\n",
    "    st_init = np.zeros((1, rnn_cell.state_size), np.float32)\n",
    "\n",
    "    # defining initial state\n",
    "    state_in = tf.placeholder(tf.float32, [1, rnn_cell.state_size])\n",
    "\n",
    "    # reshape inputs size\n",
    "    rnn_in = tf.expand_dims(inputs, [0])\n",
    "\n",
    "    step_size = tf.shape(prev_rewards)[:1]\n",
    "\n",
    "    # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "    # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "    outputs, state_out = tf.nn.dynamic_rnn(rnn_cell, rnn_in,\n",
    "                                           initial_state=state_in,\n",
    "                                           sequence_length=step_size,\n",
    "                                           dtype=tf.float32,\n",
    "                                           time_major=False)\n",
    "\n",
    "    rnn_out = tf.reshape(outputs, [-1, num_units])\n",
    "\n",
    "    actions, actions_onehot, policy, value = \\\n",
    "        process_output(rnn_out, outputs, a_size, num_units)\n",
    "\n",
    "    return st_init, state_in, state_out, actions, actions_onehot, policy, value\n",
    "\n",
    "def process_output(rnn_out, outputs, a_size, num_units):\n",
    "    # Actions\n",
    "    actions = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    actions_onehot = tf.one_hot(actions, a_size, dtype=tf.float32)\n",
    "\n",
    "    # Output layers for policy and value estimations\n",
    "    policy = slim.fully_connected(rnn_out, a_size,\n",
    "                                  activation_fn=tf.nn.softmax,\n",
    "                                  weights_initializer=normalized_columns_initializer(0.01),\n",
    "                                  biases_initializer=None)\n",
    "    value = slim.fully_connected(rnn_out, 1,\n",
    "                                 activation_fn=None,\n",
    "                                 weights_initializer=normalized_columns_initializer(1.0),\n",
    "                                 biases_initializer=None)\n",
    "\n",
    "    return actions, actions_onehot, policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class data():\n",
    "    def __init__(self, folder=''):\n",
    "        # point by point parameter mats saved for some trials\n",
    "        self.states_point = []\n",
    "        self.net_state_point = []\n",
    "        self.rewards_point = []\n",
    "        self.done_point = []\n",
    "        self.actions_point = []\n",
    "        self.corrects_point = []\n",
    "        self.new_trial_point = []\n",
    "        self.trials_point = []\n",
    "        self.stims_conf_point = []\n",
    "        # where to save the trials data\n",
    "        self.folder = folder\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset all mats\n",
    "        \"\"\"\n",
    "        # reset parameters mat\n",
    "        self.states_point = []\n",
    "        self.net_state_point = []\n",
    "        self.rewards_point = []\n",
    "        self.done_point = []\n",
    "        self.actions_point = []\n",
    "        self.corrects_point = []\n",
    "        self.new_trial_point = []\n",
    "        self.stims_conf_point = []\n",
    "        self.trials_point = []\n",
    "\n",
    "    def update(self, new_state=[], net_state=[], reward=None, update_net=None,\n",
    "               action=None, correct=[], new_trial=None, num_trials=None,\n",
    "               stim_conf=[]):\n",
    "        \"\"\"\n",
    "        append available info\n",
    "        \"\"\"\n",
    "        if len(new_state) != 0:\n",
    "            self.states_point.append(new_state)\n",
    "        if len(net_state) != 0:\n",
    "            self.net_state_point.append(net_state)\n",
    "        if reward is not None:\n",
    "            self.rewards_point.append(reward)\n",
    "        if update_net is not None:\n",
    "            self.done_point.append(update_net)  # 0 by construction\n",
    "        if action is not None:\n",
    "            self.actions_point.append(action)\n",
    "        if len(correct) != 0:\n",
    "            self.corrects_point.append(correct)\n",
    "        if new_trial is not None:\n",
    "            self.new_trial_point.append(new_trial)  # 0 by construction\n",
    "        if num_trials is not None:\n",
    "            self.trials_point.append(num_trials)\n",
    "        if len(stim_conf) != 0:\n",
    "            self.stims_conf_point.append(stim_conf)\n",
    "\n",
    "    def save(self, num_trials):\n",
    "        \"\"\"\n",
    "        save data\n",
    "        \"\"\"\n",
    "        data = {'states': self.states_point, 'net_state': self.net_state_point,\n",
    "                'rewards': self.rewards_point, 'done_flags': self.done_point,\n",
    "                'actions': self.actions_point, 'corrects': self.corrects_point,\n",
    "                'new_trial_flags': self.new_trial_point,\n",
    "                'trials_saved': self.trials_point,\n",
    "                'stims_conf': self.stims_conf_point}\n",
    "        np.savez(self.folder + '/all_points_' + str(num_trials) + '.npz',\n",
    "                 **data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class expectations():\n",
    "    def __init__(self, update_net_step=100, trial_duration=10,\n",
    "                 repeating_prob=(0.2, 0.7), rewards=(-0.1, 0.0, 1.0, -1.0),\n",
    "                 block_dur=200, stim_evidence=0.5, folder=''):\n",
    "        # every X trial, the RNN network is updated with the samples\n",
    "        # collected by the agent\n",
    "        self.update_net_step = update_net_step\n",
    "\n",
    "        # number of stim values presented. During the presentation of the stim,\n",
    "        # the net has to 'fixate'\n",
    "        self.td = trial_duration\n",
    "\n",
    "        # rewards for:\n",
    "        # [stop fixating too early, correctly keep fixating,\n",
    "        # get the stimulus right, get the stimulus wrong]\n",
    "        self.rewards = rewards\n",
    "\n",
    "        # this variable describes the distributions from which the\n",
    "        # presented stimuli are drawn (it might vary from task to task)\n",
    "        self.internal_state = []\n",
    "\n",
    "        self.num_tr = 0\n",
    "        self.num_actions = 3\n",
    "\n",
    "        # keeps track of the repeating prob of the current block\n",
    "        self.curr_rep_prob = 0\n",
    "\n",
    "        # number of trials for repeating/alternating blocks\n",
    "        self.block_dur = block_dur\n",
    "\n",
    "        # position of the stimuli\n",
    "        self.stms_pos_new_trial = 0\n",
    "\n",
    "        # stimulus evidence: one stimulus is always N(1,1), the mean of\n",
    "        # the other is drawn from a uniform distrib.=U(stim_ev,1).\n",
    "        # stim_evidence must then be between 0 and 1 and the higher it is\n",
    "        # the more difficult is the task\n",
    "        self.stim_evidence = np.max([stim_evidence, 10e-5])\n",
    "\n",
    "        # prob. of repeating the stimuli in the positions of previous trial\n",
    "        self.repeating_prob = repeating_prob\n",
    "\n",
    "        # SAVED PARAMETERS AT THE END OF THE TRIAL\n",
    "\n",
    "        # stimulus evidence\n",
    "        self.evidence_mat = []\n",
    "\n",
    "        # mean of stimulus 2 (mean of stimulus 1 is always 1)\n",
    "        self.stim2_mat = []\n",
    "\n",
    "        # position of stimulus 1\n",
    "        self.stms_pos = []\n",
    "\n",
    "        # whether the stimulus is repeating or not\n",
    "        self.repeat_mat = []\n",
    "\n",
    "        # reward\n",
    "        self.reward_mat = []\n",
    "\n",
    "        # performance\n",
    "        self.perf_mat = []\n",
    "\n",
    "        # duration of trial\n",
    "        self.dur_tr = []\n",
    "\n",
    "        # current repeating probability\n",
    "        self.rep_prob = []\n",
    "\n",
    "        # summed activity across the trial\n",
    "        self.net_smmd_act = []\n",
    "\n",
    "        # summed activity across the trial\n",
    "        self.action = []\n",
    "\n",
    "        # folder\n",
    "        self.folder = folder\n",
    "\n",
    "        # point by point parameter mats saved for some trials\n",
    "        self.all_pts_data = data(folder=folder)\n",
    "\n",
    "        # save all points step. Here I call the class data that implements\n",
    "        # all the necessary functions\n",
    "        self.sv_pts_stp = 10\n",
    "        self.num_tr_svd = 1000\n",
    "\n",
    "    def get_state(self):\n",
    "        self.timestep += 1  # this was previously in pullArm\n",
    "        if self.timestep < self.td:\n",
    "            self.state = [np.random.normal(self.int_st[0], scale=1),\n",
    "                          np.random.normal(self.int_st[1],scale=1), -1]\n",
    "        else:\n",
    "            self.state = [0, 0, 0]\n",
    "\n",
    "        self.evidence += self.state[0]-self.state[1]\n",
    "        self.state = np.reshape(self.state, [1, self.num_actions, 1])\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def new_trial(self):\n",
    "        self.num_tr += 1\n",
    "        self.timestep = 0\n",
    "        self.evidence = 0\n",
    "        stim1 = 1.0\n",
    "        stim2 = np.random.uniform(1-self.stim_evidence, 1)\n",
    "        assert stim2 != 1.0\n",
    "        a = [stim1, stim2]\n",
    "        # right now self.true is always = 1,\n",
    "        # so you could consider just removing this variable\n",
    "        self.true = a[0]\n",
    "        self.choices = a\n",
    "\n",
    "        # decide the position of the stims\n",
    "        # if the block is finished update the prob of repeating\n",
    "        if self.num_tr % self.block_dur == 0:\n",
    "            self.curr_rep_prob = int(not self.curr_rep_prob)\n",
    "\n",
    "        # flip a coin\n",
    "        repeat = np.random.uniform() < self.repeating_prob[self.curr_rep_prob]\n",
    "        if not repeat:\n",
    "            self.stms_pos_new_trial = not(self.stms_pos_new_trial)\n",
    "\n",
    "        aux = [self.choices[x] for x in [int(self.stms_pos_new_trial),\n",
    "               int(not self.stms_pos_new_trial)]]\n",
    "\n",
    "        self.int_st = np.concatenate((aux, np.array([-1])))\n",
    "\n",
    "        # store some data about trial\n",
    "        self.stim2_mat.append(stim2)\n",
    "        self.stms_pos.append(self.stms_pos_new_trial)\n",
    "        self.repeat_mat.append(repeat)\n",
    "        self.rep_prob.append(self.curr_rep_prob)\n",
    "\n",
    "        # get state\n",
    "        s = self.get_state()\n",
    "\n",
    "        # during some episodes I save all data points\n",
    "        if np.floor(self.num_tr/self.num_tr_svd) % self.sv_pts_stp == 0:\n",
    "            self.all_pts_data.update(new_state=s,\n",
    "                                     new_trial=1,\n",
    "                                     num_trials=self.num_tr)\n",
    "\n",
    "        return s\n",
    "\n",
    "    def pullArm(self, action, net_st=[]):\n",
    "        \"\"\"\n",
    "        receives an action and returns a reward, a state and flag variables\n",
    "        that indicate whether to start a new trial and whether to update\n",
    "        the network\n",
    "        \"\"\"\n",
    "        trial_dur = 0\n",
    "        new_trial = True\n",
    "        correct = False\n",
    "        update_net = False\n",
    "\n",
    "        # decide which reward and state (new_trial, correct) we are in\n",
    "        if self.timestep < self.td:\n",
    "            if (self.int_st[action] != -1).all():\n",
    "                reward = self.rewards[0]\n",
    "            else:\n",
    "                # don't abort the trial even if the network stops fixating\n",
    "                reward = self.rewards[1]\n",
    "\n",
    "            new_trial = False\n",
    "\n",
    "        else:\n",
    "            if (self.int_st[action] == self.true).all():\n",
    "                reward = self.rewards[2]\n",
    "                correct = True\n",
    "            else:\n",
    "                reward = self.rewards[3]\n",
    "            trial_dur = self.timestep\n",
    "\n",
    "        if new_trial:\n",
    "            # current trial info\n",
    "            self.dur_tr.append(trial_dur)\n",
    "            self.perf_mat.append(correct)\n",
    "            self.action.append(action)\n",
    "            self.reward_mat.append(reward)\n",
    "            self.evidence_mat.append(self.evidence)\n",
    "            new_st = None\n",
    "\n",
    "            # check if it is time to update the network\n",
    "            update_net = ((self.num_tr-1) % self.update_net_step == 0) and\\\n",
    "                (self.num_tr != 1)\n",
    "\n",
    "            # point by point parameter mats saved for some periods\n",
    "            if np.floor(self.num_tr / self.num_tr_svd) % self.sv_pts_stp == 0:\n",
    "                self.all_pts_data.update(net_state=net_st, reward=reward,\n",
    "                                         update_net=update_net,\n",
    "                                         action=action, correct=[correct])\n",
    "\n",
    "            # during some episodes I save all data points\n",
    "            aux = np.floor((self.num_tr-1) / self.num_tr_svd)\n",
    "            aux2 = np.floor(self.num_tr / self.num_tr_svd)\n",
    "            if aux % self.sv_pts_stp == 0 and\\\n",
    "               aux2 % self.sv_pts_stp == 1:\n",
    "                self.all_pts_data.save(self.num_tr)\n",
    "                self.all_pts_data.reset()\n",
    "\n",
    "        else:\n",
    "            new_st = self.get_state()\n",
    "            # during some episodes I save all data points\n",
    "            if np.floor(self.num_tr / self.num_tr_svd) % self.sv_pts_stp == 0:\n",
    "                self.all_pts_data.update(new_state=new_st, net_state=net_st,\n",
    "                                         reward=reward, update_net=update_net,\n",
    "                                         action=action, correct=[correct],\n",
    "                                         new_trial=new_trial,\n",
    "                                         num_trials=self.num_tr)\n",
    "\n",
    "        return new_st, reward, update_net, new_trial\n",
    "\n",
    "    def save_trials_data(self):\n",
    "        # Periodically save model trials statistics.\n",
    "        if self.num_tr % 10000 == 0:\n",
    "            data = {'stim2': self.stim2_mat, 'trial_duration': self.dur_tr,\n",
    "                    'stims_position': self.stms_pos, 'repeat': self.repeat_mat,\n",
    "                    'reward': self.reward_mat, 'performance': self.perf_mat,\n",
    "                    'evidence': self.evidence_mat, 'rep_prob': self.rep_prob,\n",
    "                    'net_smmd_act': self.net_smmd_act, 'action': self.action}\n",
    "            np.savez(self.folder +\n",
    "                     '/trials_stats_' + str(self.num_tr) + '.npz', **data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     1
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class AC_Network():\n",
    "    def __init__(self, a_size, state_size, scope, trainer, num_units, network):\n",
    "        with tf.variable_scope(scope):\n",
    "            # Input and visual encoding layers\n",
    "            self.st = tf.placeholder(shape=[None, 1, state_size, 1],\n",
    "                                     dtype=tf.float32)\n",
    "            self.prev_rewards = tf.placeholder(shape=[None, 1],\n",
    "                                               dtype=tf.float32)\n",
    "            self.prev_actions = tf.placeholder(shape=[None],\n",
    "                                               dtype=tf.int32)\n",
    "\n",
    "            self.prev_actions_onehot = tf.one_hot(self.prev_actions, a_size,\n",
    "                                                  dtype=tf.float32)\n",
    "\n",
    "            hidden = tf.concat([slim.flatten(self.st), self.prev_rewards,\n",
    "                                self.prev_actions_onehot], 1)\n",
    "\n",
    "            # call RNN network\n",
    "            if network == 'relu':\n",
    "                net = RNN_ReLU\n",
    "            elif network == 'lstm':\n",
    "                net = RNN\n",
    "            elif network == 'gru':\n",
    "                net = RNN_GRU\n",
    "            elif network == 'ugru':\n",
    "                net = RNN_UGRU\n",
    "            else:\n",
    "                raise ValueError('Unknown network')\n",
    "\n",
    "            self.st_init, self.st_in, self.st_out, self.actions,\\\n",
    "                self.actions_onehot, self.policy, self.value =\\\n",
    "                net(hidden, self.prev_rewards, a_size, num_units)\n",
    "\n",
    "            # Only the worker network needs ops for loss functions\n",
    "            # and gradient updating.\n",
    "            if scope != 'global':\n",
    "                self.target_v = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "                self.advantages = tf.placeholder(shape=[None],\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "                self.resp_outputs = \\\n",
    "                    tf.reduce_sum(self.policy * self.actions_onehot, [1])\n",
    "\n",
    "                # Loss functions\n",
    "                self.value_loss = 0.5 * tf.reduce_sum(\n",
    "                        tf.square(self.target_v -\n",
    "                                  tf.reshape(self.value, [-1])))\n",
    "                self.entropy = - tf.reduce_sum(\n",
    "                        self.policy * tf.log(self.policy + 1e-7))\n",
    "                self.policy_loss = -tf.reduce_sum(\n",
    "                        tf.log(self.resp_outputs + 1e-7)*self.advantages)\n",
    "                self.loss = 0.5 * self.value_loss +\\\n",
    "                    self.policy_loss -\\\n",
    "                    self.entropy * 0.05\n",
    "\n",
    "                # Get gradients from local network using local losses\n",
    "                local_vars = tf.get_collection(\n",
    "                        tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "                self.gradients = tf.gradients(self.loss, local_vars)\n",
    "                self.var_norms = tf.global_norm(local_vars)\n",
    "                grads, self.grad_norms =\\\n",
    "                    tf.clip_by_global_norm(self.gradients, 999.0)\n",
    "\n",
    "                # Apply local gradients to global network\n",
    "                global_vars = tf.get_collection(\n",
    "                        tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "                self.apply_grads = trainer.apply_gradients(\n",
    "                        zip(grads, global_vars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "    def __init__(self, game, name, a_size, state_size, trainer,\n",
    "                 model_path, global_epss, data_path, num_units, network):\n",
    "        self.name = \"worker_\" + str(name)\n",
    "        self.number = name\n",
    "        self.folder = './' + data_path + '/trains/train_' + str(self.number)\n",
    "        self.model_path = model_path\n",
    "        self.trainer = trainer\n",
    "        self.global_epss = global_epss\n",
    "        self.increment = self.global_epss.assign_add(1)\n",
    "        self.network = network\n",
    "        self.eps_rewards = []\n",
    "        self.eps_mean_values = []\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(self.folder)\n",
    "\n",
    "        # Create the local copy of the network and the tensorflow op\n",
    "        # to copy global parameters to local network\n",
    "        self.local_AC = AC_Network(a_size, state_size, self.name, trainer,\n",
    "                                   num_units, network)\n",
    "        self.update_local_ops = update_target_graph('global', self.name)\n",
    "        self.env = game\n",
    "\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
    "        rollout = np.array(rollout)\n",
    "        states = rollout[:, 0]\n",
    "        actions = rollout[:, 1]\n",
    "        rewards = rollout[:, 2]\n",
    "\n",
    "        prev_rewards = [0] + rewards[:-1].tolist()\n",
    "        prev_actions = [0] + actions[:-1].tolist()\n",
    "        values = rollout[:, 3]\n",
    "\n",
    "        self.pr = prev_rewards\n",
    "        self.pa = prev_actions\n",
    "        # Here we take the rewards and values from the rollout, and use them to\n",
    "        # generate the advantage and discounted returns.\n",
    "        # The advantage function uses \"Generalized Advantage Estimation\"\n",
    "        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])\n",
    "        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = rewards +\\\n",
    "            gamma * self.value_plus[1:] -\\\n",
    "            self.value_plus[:-1]\n",
    "        advantages = discount(advantages, gamma)\n",
    "\n",
    "        # Update the global network using gradients from loss\n",
    "        # Generate network statistics to periodically save\n",
    "        rnn_state = self.local_AC.st_init\n",
    "        if self.network == 'lstm':\n",
    "            feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                         self.local_AC.state: np.stack(states, axis=0),\n",
    "                         self.local_AC.prev_rewards: np.vstack(prev_rewards),\n",
    "                         self.local_AC.prev_actions: prev_actions,\n",
    "                         self.local_AC.actions: actions,\n",
    "                         self.local_AC.advantages: advantages,\n",
    "                         self.local_AC.state_in[0]: rnn_state[0],\n",
    "                         self.local_AC.state_in[1]: rnn_state[1]}\n",
    "        elif (self.network == 'relu') or\\\n",
    "             (self.network == 'gru') or\\\n",
    "             (self.network == 'ugru'):\n",
    "            feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                         self.local_AC.st: np.stack(states, axis=0),\n",
    "                         self.local_AC.prev_rewards: np.vstack(prev_rewards),\n",
    "                         self.local_AC.prev_actions: prev_actions,\n",
    "                         self.local_AC.actions: actions,\n",
    "                         self.local_AC.advantages: advantages,\n",
    "                         self.local_AC.st_in: rnn_state}\n",
    "\n",
    "        v_l, p_l, e_l, g_n, v_n, _ = sess.run([self.local_AC.value_loss,\n",
    "                                               self.local_AC.policy_loss,\n",
    "                                               self.local_AC.entropy,\n",
    "                                               self.local_AC.grad_norms,\n",
    "                                               self.local_AC.var_norms,\n",
    "                                               self.local_AC.apply_grads],\n",
    "                                              feed_dict=feed_dict)\n",
    "        aux = len(rollout)\n",
    "        return v_l / aux, p_l / aux, e_l / aux, g_n, v_n\n",
    "\n",
    "    def work(self, gamma, sess, coord, saver, train, exp_dur):\n",
    "        eps_count = sess.run(self.global_epss)\n",
    "        num_eps_tr_stats = int(1000/self.env.update_net_step)\n",
    "        num_epss_end = int(exp_dur/self.env.update_net_step)\n",
    "        num_epss_save_model = int(5000/self.env.update_net_step)\n",
    "        total_steps = 0\n",
    "        print(\"Starting worker \" + str(self.number))\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                eps_buffer = []\n",
    "                eps_values = []\n",
    "                eps_reward = 0\n",
    "                eps_step_count = 0\n",
    "                d = False\n",
    "                r = 0\n",
    "                a = 0\n",
    "\n",
    "                # get first state\n",
    "                s = self.env.new_trial()\n",
    "\n",
    "                rnn_state = self.local_AC.st_init\n",
    "                net_smmd_act = np.zeros_like(rnn_state)\n",
    "                while not d:\n",
    "                    if self.network == 'lstm':\n",
    "                        feed_dict = {\n",
    "                                    self.local_AC.state: [s],\n",
    "                                    self.local_AC.prev_rewards: [[r]],\n",
    "                                    self.local_AC.prev_actions: [a],\n",
    "                                    self.local_AC.state_in[0]: rnn_state[0],\n",
    "                                    self.local_AC.state_in[1]: rnn_state[1]}\n",
    "                    elif (self.network == 'relu') or\\\n",
    "                         (self.network == 'gru') or\\\n",
    "                         (self.network == 'ugru'):\n",
    "                        feed_dict = {\n",
    "                                    self.local_AC.st: [s],\n",
    "                                    self.local_AC.prev_rewards: [[r]],\n",
    "                                    self.local_AC.prev_actions: [a],\n",
    "                                    self.local_AC.st_in: rnn_state}\n",
    "\n",
    "                    # Take an action using probs from policy network output\n",
    "                    a_dist, v, rnn_state_new = sess.run(\n",
    "                                                        [self.local_AC.policy,\n",
    "                                                         self.local_AC.value,\n",
    "                                                         self.local_AC.st_out],\n",
    "                                                        feed_dict=feed_dict)\n",
    "\n",
    "                    a = np.random.choice(a_dist[0], p=a_dist[0])\n",
    "                    a = np.argmax(a_dist == a)\n",
    "                    rnn_state = rnn_state_new\n",
    "                    net_smmd_act += rnn_state_new\n",
    "                    aux = np.floor(self.env.num_tr/self.env.num_tr_svd)\n",
    "                    if aux % self.env.sv_pts_stp == 0:\n",
    "                        network_activity = rnn_state_new\n",
    "                    else:\n",
    "                        network_activity = []\n",
    "                    # new_state, reward, update_net, new_trial\n",
    "                    s1, r, d, nt = self.env.pullArm(a, network_activity)\n",
    "                    # save samples for training the network later\n",
    "                    eps_buffer.append([s, a, r, v[0, 0]])\n",
    "                    eps_values.append(v[0, 0])\n",
    "                    eps_reward += r\n",
    "                    total_steps += 1\n",
    "                    eps_step_count += 1\n",
    "                    # store the summed activity at the end of the trial\n",
    "                    if nt:\n",
    "                        self.env.net_smmd_act.append(net_smmd_act)\n",
    "                        net_smmd_act = np.zeros_like(rnn_state)\n",
    "                        self.env.save_trials_data()\n",
    "                    if not d:\n",
    "                        if nt:\n",
    "                            s = self.env.new_trial()\n",
    "                        else:\n",
    "                            s = s1\n",
    "\n",
    "                self.eps_rewards.append(eps_reward)\n",
    "                self.eps_mean_values.append(np.mean(eps_values))\n",
    "\n",
    "                # Update the network using the experience buffer\n",
    "                # at the end of the episode\n",
    "                if len(eps_buffer) != 0 and train:\n",
    "                    v_l, p_l, e_l, g_n, v_n = \\\n",
    "                        self.train(eps_buffer, sess, gamma, 0.0)\n",
    "\n",
    "                # Periodically save model parameters and summary statistics.\n",
    "                if eps_count % num_eps_tr_stats == 0 and eps_count != 0:\n",
    "                    if eps_count % num_epss_save_model == 0 and\\\n",
    "                       self.name == 'worker_0' and\\\n",
    "                       train and\\\n",
    "                       len(self.eps_rewards) != 0:\n",
    "                        saver.save(sess, self.model_path +\n",
    "                                   '/model-' + str(eps_count) + '.cptk')\n",
    "\n",
    "                    mean_tr_dur = np.mean(self.env.dur_tr[-10:])\n",
    "                    mean_reward = np.mean(self.eps_rewards[-10:])\n",
    "                    mean_value = np.mean(self.eps_mean_values[-10:])\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Perf/trial_duration',\n",
    "                                      simple_value=float(mean_tr_dur))\n",
    "                    summary.value.add(tag='Perf/Reward',\n",
    "                                      simple_value=float(mean_reward))\n",
    "                    summary.value.add(tag='Perf/Value',\n",
    "                                      simple_value=float(mean_value))\n",
    "\n",
    "                    performance_aux = np.vstack(np.array(self.env.perf_mat))\n",
    "\n",
    "                    for ind_crr in range(performance_aux.shape[1]):\n",
    "                        mean_performance = np.mean(performance_aux[:, ind_crr])\n",
    "                        summary.value.add(tag='Perf/Perf_' + str(ind_crr),\n",
    "                                          simple_value=float(mean_performance))\n",
    "\n",
    "                    if train:\n",
    "                        summary.value.add(tag='Losses/Value Loss',\n",
    "                                          simple_value=float(v_l))\n",
    "                        summary.value.add(tag='Losses/Policy Loss',\n",
    "                                          simple_value=float(p_l))\n",
    "                        summary.value.add(tag='Losses/Entropy',\n",
    "                                          simple_value=float(e_l))\n",
    "                        summary.value.add(tag='Losses/Grad Norm',\n",
    "                                          simple_value=float(g_n))\n",
    "                        summary.value.add(tag='Losses/Var Norm',\n",
    "                                          simple_value=float(v_n))\n",
    "                    self.summary_writer.add_summary(summary, eps_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "\n",
    "                eps_count += 1\n",
    "                if eps_count > num_epss_end:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "def main_priors(load_model=False, train=True, gamma=.8, update_net_step=5,\n",
    "                trial_duration=10, repeating_prob=(0.2, 0.8), exp_dur=10**6,\n",
    "                rewards=(-0.1, 0.0, 1.0, -1.0), block_dur=200,\n",
    "                num_units=32, stim_evidence=.3, network='ugru',\n",
    "                learning_rate=1e-3, instance=0):\n",
    "    a_size = 3  # number of actions\n",
    "    state_size = a_size  # number of inputs\n",
    "    if train:\n",
    "        test_flag = ''\n",
    "    else:\n",
    "        test_flag = '_test'\n",
    "    data_path = 'priors/' + 'trial_duration_' + str(trial_duration) +\\\n",
    "        '_repeating_prob_' + str(list_str(repeating_prob)) +\\\n",
    "        '_rewards_' + str(list_str(rewards)) +\\\n",
    "        '_block_dur_' + str(block_dur) + '_stimEv_' + str(stim_evidence) +\\\n",
    "        '_gamma_' + str(gamma) + '_num_units_' + str(num_units) +\\\n",
    "        '_update_net_step_' + str(update_net_step) + '_network_' \\\n",
    "        + str(network) + '_' + str(instance) + test_flag + '/'\n",
    "\n",
    "    data = {'trial_duration': trial_duration, 'repeating_prob': repeating_prob,\n",
    "            'rewards': rewards, 'stim_evidence': stim_evidence,\n",
    "            'block_dur': block_dur, 'gamma': gamma, 'num_units': num_units,\n",
    "            'update_net_step': update_net_step, 'network': network}\n",
    "\n",
    "    model_path = './' + data_path + '/model_meta_context'\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    np.savez(data_path + '/experiment_setup.npz', **data)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        global_episodes = tf.Variable(0, dtype=tf.int32,\n",
    "                                      name='global_episodes',\n",
    "                                      trainable=False)\n",
    "        trainer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        AC_Network(a_size, state_size, 'global',\n",
    "                             None, num_units, network)  # Generate global net\n",
    "        # Set workers to number of available CPU threads\n",
    "        num_workers = multiprocessing.cpu_count()\n",
    "        workers = []\n",
    "        # Create worker classes\n",
    "        for i in range(num_workers):\n",
    "            saving_path = './' + data_path + '/trains/train_' + str(i)\n",
    "            workers.append(Worker(expectations(\n",
    "                                    update_net_step=update_net_step,\n",
    "                                    trial_duration=trial_duration,\n",
    "                                    repeating_prob=repeating_prob,\n",
    "                                    rewards=rewards, block_dur=block_dur,\n",
    "                                    stim_evidence=stim_evidence,\n",
    "                                    folder=saving_path),\n",
    "                            i, a_size, state_size,\n",
    "                            trainer, model_path, global_episodes,\n",
    "                            data_path, num_units, network))\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        if load_model:\n",
    "            print('Loading Model...')\n",
    "            print(model_path)\n",
    "            ckpt = tf.train.get_checkpoint_state(model_path)\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        worker_threads = []\n",
    "        for worker in workers:\n",
    "            worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
    "            thread = threading.Thread(target=(worker_work))\n",
    "            thread.start()\n",
    "            worker_threads.append(thread)\n",
    "        coord.join(worker_threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosyne_priors.ipynb  gym_priors.ipynb  \u001b[0m\u001b[01;34mpriors\u001b[0m/           Untitled.ipynb\r\n",
      "\u001b[01;34mgym-priors\u001b[0m/          \u001b[01;34mhome\u001b[0m/             priors_2.0.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting worker 0\n",
      "Starting worker 3\n",
      "Starting worker 4\n",
      "Starting worker 5\n",
      "Starting worker 6\n",
      "Starting worker 11\n",
      "Starting worker 1\n",
      "Starting worker 10\n",
      "Starting worker 9\n",
      "Starting worker 2\n",
      "Starting worker 8Starting worker 7\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n",
      "\n",
      "Exception in thread Thread-23:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Session has been closed.\n",
      "Exception in thread Thread-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-21:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "\n",
      "Exception in thread Thread-20:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "\n",
      "Exception in thread Thread-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "Exception in thread Thread-24:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "\n",
      "Exception in thread Thread-25:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 119, in work\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n",
      "Exception in thread Thread-27:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
      "    return fn(*args)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
      "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
      "    run_metadata)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-7-f94e668ab927>\", line 69, in <lambda>\n",
      "    worker_work = lambda: worker.work(gamma, sess, coord, saver, train, exp_dur)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 155, in work\n",
      "    v_l, p_l, e_l, g_n, v_n =                         self.train(eps_buffer, sess, gamma, 0.0)\n",
      "  File \"<ipython-input-6-12e170ec2e4a>\", line 72, in train\n",
      "    feed_dict=feed_dict)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 929, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
      "    feed_dict_tensor, options, run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
      "    run_metadata)\n",
      "  File \"/home/molano/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
      "    raise type(e)(node_def, op, message)\n",
      "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3ed92d786b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                 \u001b[0mrewards\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_dur\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                 \u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstim_evidence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ugru'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 learning_rate=1e-3, instance=123)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f94e668ab927>\u001b[0m in \u001b[0;36mmain_priors\u001b[0;34m(load_model, train, gamma, update_net_step, trial_duration, repeating_prob, exp_dur, rewards, block_dur, num_units, stim_evidence, network, learning_rate, instance)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mworker_threads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Wait for all threads to stop or for request_stop() to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mwait_for_stop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCoordinator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtold\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mregister_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_priors(load_model=False, train=True, gamma=.8, update_net_step=5,\n",
    "                trial_duration=10, repeating_prob=(0.2, 0.8),\n",
    "                rewards=(-0.1, 0.0, 1.0, -1.0), block_dur=200,\n",
    "                num_units=32, stim_evidence=.4, network='ugru',\n",
    "                learning_rate=1e-3, instance=123)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12.2333px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "180px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
